"""
Script para Regenerar ChromaDB con Datos Limpios
Elimina chunks problem√°ticos y regenera la base de datos vectorial
"""

import pandas as pd
import shutil
import os
from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
import re

print("=" * 80)
print("üîÑ REGENERACI√ìN DE BASE DE DATOS VECTORIAL")
print("=" * 80)

# ===== PASO 1: CARGAR DATOS ORIGINALES =====
print("\nüìÇ Paso 1: Cargando datos originales...")

df1 = pd.read_csv("./data/chunks/celsia_processed_20251015_223656_chunks.csv")
df2 = pd.read_csv("./data/chunks/post_celsia_chunks.csv")

df1 = df1[['Contenido_Completo']].rename(columns={'Contenido_Completo': 'chunk'})
df2 = df2[['chunk']]
df = pd.concat([df1, df2], ignore_index=True)

print(f"‚úÖ Se cargaron {len(df)} chunks totales")

# ===== PASO 2: LIMPIAR DATOS =====
print("\nüßπ Paso 2: Limpiando datos...")

# Funci√≥n de limpieza mejorada
def limpiar_texto_para_rag(texto):
    if pd.isna(texto) or not isinstance(texto, str):
        return ""
    
    # a) Eliminar emojis
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"
        "\U0001F300-\U0001F5FF"
        "\U0001F680-\U0001F6FF"
        "\U0001F900-\U0001F9FF"
        "\U00002702-\U000027B0"
        "]+", flags=re.UNICODE)
    texto = emoji_pattern.sub(r'', texto)
    
    # Eliminar caracteres especiales manteniendo puntuaci√≥n b√°sica
    texto = re.sub(r'[^a-zA-Z0-9\s√±√°√©√≠√≥√∫√º√ë√Å√â√ç√ì√ö√ú.,;:¬ø?¬°!()-]', '', texto)
    
    # Eliminar patrones repetitivos
    patron_frase_completa = r'\bEdici√≥n\s*\d+\s*Tolima\b\.?'
    texto = re.sub(patron_frase_completa, '', texto, flags=re.IGNORECASE)
    
    # Eliminar palabras sueltas problem√°ticas
    palabras_sueltas_a_eliminar = ['hashtag', 'undefined']
    patron_palabras_sueltas = r'\b(' + '|'.join(palabras_sueltas_a_eliminar) + r')\b'
    texto = re.sub(patron_palabras_sueltas, '', texto, flags=re.IGNORECASE)

    # Limpieza final: eliminar espacios extra
    texto = re.sub(r'\s+', ' ', texto).strip()
    
    return texto

# Aplicar limpieza
df['chunk_limpio'] = df['chunk'].apply(limpiar_texto_para_rag)

# ===== PASO 3: FILTRAR CHUNKS PROBLEM√ÅTICOS =====
print("\nüîç Paso 3: Filtrando chunks problem√°ticos...")

inicial = len(df)

# 1. Eliminar chunks vac√≠os o muy cortos (< 30 caracteres)
df = df[df['chunk_limpio'].str.len() >= 30]
print(f"  - Eliminados {inicial - len(df)} chunks muy cortos (<30 chars)")

# 2. Eliminar duplicados exactos
inicial = len(df)
df = df.drop_duplicates(subset=['chunk_limpio'])
print(f"  - Eliminados {inicial - len(df)} chunks duplicados")

# 3. Eliminar chunks que solo tienen palabras repetitivas o sin sentido
def es_chunk_valido(texto):
    # Verificar que tenga al menos 3 palabras diferentes
    palabras = texto.split()
    if len(set(palabras)) < 3:
        return False
    # Verificar que tenga al menos una palabra de m√°s de 4 letras
    if not any(len(p) > 4 for p in palabras):
        return False
    return True

inicial = len(df)
df = df[df['chunk_limpio'].apply(es_chunk_valido)]
print(f"  - Eliminados {inicial - len(df)} chunks sin contenido significativo")

print(f"\n‚úÖ Total de chunks limpios: {len(df)}")

# Estad√≠sticas finales
print(f"\nüìä Estad√≠sticas de longitud despu√©s de limpieza:")
print(f"  Min: {df['chunk_limpio'].str.len().min()} chars")
print(f"  Max: {df['chunk_limpio'].str.len().max()} chars")
print(f"  Media: {df['chunk_limpio'].str.len().mean():.0f} chars")
print(f"  Mediana: {df['chunk_limpio'].str.len().median():.0f} chars")

# ===== PASO 4: ELIMINAR CHROMADB ANTIGUA =====
print("\nüóëÔ∏è Paso 4: Eliminando base de datos anterior...")

chromadb_path = "./chromadb_storage"
if os.path.exists(chromadb_path):
    try:
        shutil.rmtree(chromadb_path)
        print(f"‚úÖ Directorio {chromadb_path} eliminado")
    except Exception as e:
        print(f"‚ö†Ô∏è Error al eliminar: {e}")
        print("   Cierra cualquier proceso que est√© usando ChromaDB y reintenta")
        exit(1)
else:
    print(f"‚ÑπÔ∏è No existe base de datos anterior")

# ===== PASO 5: CREAR DOCUMENTOS =====
print("\nüìÑ Paso 5: Creando documentos para LangChain...")

documentos = []
for i, row in df.iterrows():
    doc = Document(
        page_content=row['chunk_limpio'],
        metadata={
            "source": f"chunk_{i}",
            "length": len(row['chunk_limpio'])
        }
    )
    documentos.append(doc)

print(f"‚úÖ Se crearon {len(documentos)} documentos")

# ===== PASO 6: CONFIGURAR EMBEDDINGS =====
print("\nüî¢ Paso 6: Configurando embeddings...")

embeddings = OllamaEmbeddings(
    model="nomic-embed-text",
    base_url="http://localhost:11434"
)

print("‚úÖ Embeddings configurados")

# ===== PASO 7: CREAR Y PERSISTIR VECTORSTORE =====
print("\nüíæ Paso 7: Creando nueva base de datos vectorial...")
print("   (Esto puede tardar varios minutos dependiendo del tama√±o)")

try:
    vectorstore = Chroma.from_documents(
        documents=documentos,
        embedding=embeddings,
        persist_directory=chromadb_path,
        collection_name="rag_collection"
    )
    print(f"‚úÖ ChromaDB creado exitosamente en {chromadb_path}")
except Exception as e:
    print(f"‚ùå Error al crear ChromaDB: {e}")
    exit(1)

# ===== PASO 8: VERIFICACI√ìN =====
print("\nüîç Paso 8: Verificando la nueva base de datos...")

# Verificar conteo
total_docs = vectorstore._collection.count()
print(f"‚úÖ Documentos en la nueva BD: {total_docs}")

# Probar algunas consultas
preguntas_prueba = [
    "¬øQu√© es Celsia?",
    "¬øC√≥mo funciona la facturaci√≥n?",
    "¬øQu√© es la energ√≠a solar?",
]

print("\nüìù Probando recuperaci√≥n de documentos:")
chunks_recuperados = set()

for pregunta in preguntas_prueba:
    docs = vectorstore.similarity_search_with_score(pregunta, k=3)
    print(f"\n  ‚ùì {pregunta}")
    for i, (doc, score) in enumerate(docs, 1):
        source = doc.metadata.get('source', 'unknown')
        chunks_recuperados.add(source)
        content_preview = doc.page_content[:80].replace('\n', ' ')
        print(f"     [{i}] {source} - Score: {score:.2f}")
        print(f"         '{content_preview}...'")

print(f"\n‚úÖ Se recuperaron {len(chunks_recuperados)} chunks √∫nicos de 3 consultas")

if len(chunks_recuperados) > 3:
    print("üéâ ¬°Excelente! La base de datos ahora tiene m√°s diversidad")
else:
    print("‚ö†Ô∏è A√∫n hay poca diversidad. Puede ser necesario revisar los datos originales")

# ===== PASO 9: GUARDAR DATOS LIMPIOS (OPCIONAL) =====
print("\nüíæ Paso 9: Guardando chunks limpios para referencia...")

df_export = df[['chunk_limpio']].rename(columns={'chunk_limpio': 'chunk'})
df_export.to_csv("./data/chunks/chunks_limpios.csv", index=False)
print("‚úÖ Chunks limpios guardados en ./data/chunks/chunks_limpios.csv")

print("\n" + "=" * 80)
print("‚úÖ REGENERACI√ìN COMPLETADA EXITOSAMENTE")
print("=" * 80)
print("\nüí° Siguiente paso: Ejecuta 'streamlit run app.py' y prueba el agente")
