"""
Interfaz de Agente RAG con Streamlit y LangGraph
El Agente tiene acceso a una herramienta de b√∫squeda RAG (ChromaDB)
y una herramienta para obtener un n√∫mero de tel√©fono.
"""

import streamlit as st
import warnings

# --- Importaciones Clave de LangChain ---
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama # ChatOllama es necesario para Agentes
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import AIMessage, HumanMessage
from langchain.tools import tool # El decorador para crear herramientas
from langchain.agents import create_agent
from langgraph.checkpoint.memory import InMemorySaver 

# Ignorar advertencias (opcional, para limpiar la consola)
warnings.filterwarnings("ignore", category=UserWarning)


# --- HERRAMIENTA 1: Funci√≥n de Python Simple ---
# Usamos el decorador @tool para que el agente pueda "ver" esta funci√≥n
@tool
def get_telefono_celsia():
    """Funcion para obtener el telefono de celsia. 
    Usar S√ìLO si el usuario pide expl√≠citamente el n√∫mero de tel√©fono."""
    return "3102226655"

@tool
def get_direccion_celsia():
    """Funcion para obtener la direccion de celsia. 
    Usar S√ìLO si el usuario pide expl√≠citamente la direcci√≥n."""
    return "Calle 1 Casa 1 avenida siempre viva"


# --- Configuraci√≥n de Estado de Streamlit (Session State) ---

# Checkpointer: Guarda el estado de la conversaci√≥n (memoria) del agente
if "checkpointer" not in st.session_state:
    st.session_state.checkpointer = InMemorySaver()

# Thread ID: Un ID fijo para que la conversaci√≥n sea continua
if "thread_id" not in st.session_state:
    st.session_state.thread_id = "1" 

# Historial de mensajes para mostrar en la UI de Streamlit
if "messages" not in st.session_state:
    st.session_state.messages = []


# --- CONFIGURACI√ìN DE LA P√ÅGINA de Streamlit ---
st.set_page_config(
    page_title="Celsia Agent",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Titulo
st.markdown("<h1 style='text_align: center;'>ü§ñ Agente Celsia llama 3.2 3B</h1>", unsafe_allow_html=True)
st.write("En que puedo ayudarte hoy?")


# --- FUNCI√ìN PRINCIPAL DE CARGA (Cacheada) ---
# @st.cache_resource asegura que esto solo se ejecute una vez
@st.cache_resource
def cargar_agente_y_rag():
    """
    Esta funci√≥n carga todos los componentes (VectorDB, LLM, RAG chain)
    y construye el Agente final.
    """
    
    with st.spinner("‚è≥ Iniciando sistemas... (esto puede tardar un momento)"):
        
        # --- 1. Configuraci√≥n del LLM ---
        # Usamos ChatOllama para que el agente pueda tener conversaciones
        llm = ChatOllama(
            model="qwen3:4b",
            base_url="http://localhost:11434",
            temperature=0.3 # Temperatura baja para que el agente sea m√°s predecible
        )

        # --- 2. Configuraci√≥n de Embeddings y Vectorstore (Retriever) ---
        embeddings = OllamaEmbeddings(
            model="nomic-embed-text",
            base_url="http://localhost:11434"
        )
        
        vectorstore = Chroma(
            persist_directory="./chromadb_storage",
            embedding_function=embeddings,
            collection_name="rag_collection"
        )
        
        # El 'retriever' es la parte que busca en la base de datos
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3} # Traer los 3 fragmentos m√°s relevantes
        )
        
        # --- 3. Definici√≥n de la Cadena RAG ---
        # Este es el "cerebro" de nuestra herramienta de b√∫squeda.
        
        prompt_rag = PromptTemplate(
            template="""**[INSTRUCCIONES CLAVE ZERO-SHOT Y LIMITACI√ìN DE FUENTE]**
Tu √öNICA tarea es responder a la **PREGUNTA** del usuario, utilizando EXCLUSIVAMENTE la informaci√≥n que se encuentra en el **CONTEXTO** proporcionado a continuaci√≥n.

**REGLAS ESTRICTAS para evitar alucinaciones:**
1.  **SI** la respuesta a la PREGUNTA se encuentra expl√≠cita o impl√≠citamente en el **CONTEXTO**, genera una respuesta completa y profesional.
2.  **SI** no puedes encontrar la respuesta en el **CONTEXTO**, o si la informaci√≥n es insuficiente, debes responder **√öNICAMENTE** con la siguiente frase predefinida: "Lamento no poder ofrecer una respuesta precisa basada en la informaci√≥n disponible. Por favor, consulta los canales oficiales de CELSIA o llama a la l√≠nea de servicio al cliente."
3.  **NUNCA** utilices tu conocimiento general o informaci√≥n que no est√© en el **CONTEXTO**. **NUNCA** inventes tarifas, fechas o procesos.
        
Contexto:
{context}

Pregunta: {question}

Respuesta:""",
            input_variables=["context", "question"]
        )
        
        def formato_docs(docs):
            return "\n\n".join([doc.page_content for doc in docs])
        
        # Definimos la cadena RAG completa
        rag_chain = (
            {"context": retriever | formato_docs, "question": RunnablePassthrough()}
            | prompt_rag
            | llm # El mismo LLM puede usarse para la cadena RAG
            | StrOutputParser() # Nos aseguramos que la salida sea un string
        )

        # --- 4. HERRAMIENTA 2: La Cadena RAG como Herramienta ---
        # Aqu√≠ convertimos nuestra 'rag_chain' en una herramienta que el agente puede usar
        @tool
        def BuscadorDocumentosCelsia(pregunta: str) -> str:
            """
            Herramienta para buscar informaci√≥n en la base de datos de Celsia.
            """
            return rag_chain.invoke(pregunta)

        # --- 5. Creaci√≥n del Agente ---
        
        # Lista de herramientas que el agente puede elegir
        tools = [get_direccion_celsia, get_telefono_celsia, BuscadorDocumentosCelsia]
        
        # Prompt del sistema: Las instrucciones maestras para el Agente
        system_prompt_agent = """Eres un asistente virtual de Celsia.
Tu trabajo es ayudar a los usuarios con sus preguntas. Responde siempre en espa√±ol.
Tienes dos herramientas a tu disposici√≥n:

1.  `get_telefono_celsia`: √ösala S√ìLO si el usuario pide el n√∫mero de tel√©fono.
2.  `get_direccion_celsia`: √ösala S√ìLO si el usuario pide la direcci√≥n.
3.  `BuscadorDocumentosCelsia`: √ösala para responder CUALQUIER OTRA pregunta sobre Celsia, ya que busca en la base de datos oficial.

Primero, analiza la pregunta del usuario. Luego, elige la herramienta correcta para responder."""
        
        # Creamos el Agente
        agent_graph = create_agent(
            llm, # El LLM que tomar√° las decisiones
            tools=tools, # La lista de herramientas que puede usar
            system_prompt=system_prompt_agent,
            checkpointer=st.session_state.checkpointer,
        )
        
        # Devolvemos el agente (ya no la 'rag_chain') y el retriever (para depurar)
        return agent_graph, retriever, vectorstore

# --- Carga Inicial ---
# Llamamos a la funci√≥n. Streamlit la cachear√°.
try:
    agent_graph, retriever, vectorstore = cargar_agente_y_rag()
    st.success("‚úÖ Agente y base de datos cargados correctamente")
except Exception as e:
    st.error(f"Error al cargar el agente: {e}")
    st.stop() # Detiene la ejecuci√≥n si hay un error cr√≠tico


# --- SIDEBAR ---
with st.sidebar:
    st.header("‚öôÔ∏è Configuraci√≥n")
    try:
        st.info(f"üìä Documentos en la BD: {vectorstore._collection.count()}")
    except Exception as e:
        st.warning("No se pudo contar los documentos de ChromaDB.")
    
    if st.button("üóëÔ∏è Limpiar historial"):
        st.session_state.messages = []
        # Limpiamos tambi√©n la memoria del agente
        st.session_state.checkpointer = InMemorySaver()
        st.rerun() # Recarga la p√°gina
    
    st.divider()
    st.subheader("‚ÑπÔ∏è Informaci√≥n")
    st.write("""
    - **Modelo LLM**: Llama 3.2 3B
    - **Embeddings**: nomic-embed-text
    - **Base de datos**: ChromaDB
    - **Framework**: LangGraph (Agente)
    """)

# --- L√≥gica del Chat ---

# Mostrar historial de mensajes (de st.session_state)
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Entrada del usuario
pregunta = st.chat_input("Escribe tu pregunta...")

if pregunta:
    # 1. Mostrar el mensaje del usuario en la UI
    st.session_state.messages.append({"role": "user", "content": pregunta})
    with st.chat_message("user"):
        st.markdown(pregunta)
    
    # 2. Generar respuesta del Agente
    with st.chat_message("assistant"):
        with st.spinner("‚è≥ Pensando..."):
            
            # Configuraci√≥n para el hilo de memoria de LangGraph
            config = {"configurable": {"thread_id": st.session_state.thread_id}}
            
            # El input para el agente es un diccionario con la lista de mensajes
            input_data = {"messages": [HumanMessage(content=pregunta)]}
            
            # 3. Invocamos el Agente
            # 'respuesta_dict' es el objeto de estado final de LangGraph
            respuesta_dict = agent_graph.invoke(input_data, config=config)
            
            # 4. Extraemos la respuesta final del agente
            # La respuesta √∫til est√° en el √∫ltimo mensaje de la lista 'messages'
            respuesta_final_str = ""
            if "messages" in respuesta_dict and respuesta_dict["messages"]:
                last_message = respuesta_dict["messages"][-1]
                if isinstance(last_message, AIMessage):
                    respuesta_final_str = last_message.content
                else:
                    respuesta_final_str = "Error: El agente no produjo una respuesta final."
            else:
                respuesta_final_str = "Error: El agente no devolvi√≥ ning√∫n mensaje."
            
            # 5. Mostrar la respuesta en la UI
            st.markdown(respuesta_final_str)
            
            # 6. Guardar la respuesta en el historial de Streamlit
            st.session_state.messages.append({
                "role": "assistant",
                "content": respuesta_final_str
            })
            
            # 7. (Opcional) Mostrar documentos para depuraci√≥n
            with st.expander("üìö Documentos recuperados (Solo para referencia)"):
                # Mostramos lo que el retriever habr√≠a encontrado
                docs = retriever.invoke(pregunta)
                if docs:
                    for i, doc in enumerate(docs, 1):
                        source = doc.metadata.get('source', 'Fuente desconocida')
                        st.write(f"**[{i}] {source}**")
                        st.info(doc.page_content)
                else:
                    st.write("No se recuperaron documentos para esta consulta.")
