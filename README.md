# ğŸš€ Agente CELSIA - FastAPI con Function Calling y Google Generative AI Embeddings

**Empresa Asignada:** CELSIA  
**Grupo:** 1  
**Integrantes:**    
Estudiante: Jonathan Giraldo Diaz Ortega - cod 22501577   
Estudiante: Jhon Stiven Loaiza Rodriguez - cod 22500235   
Estudiante: Eliphas Levi Arias Abrahan - cod 22500217   
Estudiante: Juan Manuel Cajigas Eraso - cod 22500447    
**MÃ³dulo:** Capa de Conocimiento y ProductizaciÃ³n del Agente  

---

## 1ï¸âƒ£ AsignaciÃ³n de Empresa y AnÃ¡lisis Inicial (InvestigaciÃ³n)

### ğŸ”¹ AsignaciÃ³n
La empresa asignada es **CELSIA**, compaÃ±Ã­a del **Grupo Argos**, dedicada a la generaciÃ³n, transmisiÃ³n y comercializaciÃ³n de energÃ­a elÃ©ctrica en Colombia, con un enfoque estratÃ©gico en **energÃ­as renovables y eficiencia energÃ©tica**.  

### ğŸ”¹ InvestigaciÃ³n y Fuentes Consultadas
Se realizÃ³ una investigaciÃ³n exhaustiva en las siguientes fuentes de informaciÃ³n pÃºblica y digital:  

- **Sitio web oficial:** [https://www.celsia.com/](https://www.celsia.com/)  
  - Secciones analizadas: *QuiÃ©nes somos, Estrategia sostenible, Soluciones energÃ©ticas, Noticias, Inversionistas y AtenciÃ³n al cliente*.  
- **Perfil oficial en LinkedIn:** [https://www.linkedin.com/company/celsiaenergia/](https://www.linkedin.com/company/celsiaenergia/)  
  - Publicaciones sobre proyectos de innovaciÃ³n, energÃ­as limpias, reconocimientos, convocatorias laborales y acciones de sostenibilidad.  

### ğŸ”¹ DefiniciÃ³n del Alcance
El sistema Q&A debe responder preguntas frecuentes que un usuario o cliente podrÃ­a hacer al interactuar por primera vez con la empresa.  

Ejemplos de temas incluidos en el alcance:

| CategorÃ­a | Tipo de Pregunta |
|------------|------------------|
| InformaciÃ³n general | Â¿QuÃ© es Celsia?, Â¿A quÃ© grupo empresarial pertenece?, Â¿DÃ³nde opera? |
| Servicios | Â¿QuÃ© soluciones de energÃ­a ofrece?, Â¿CÃ³mo puedo generar energÃ­a solar con Celsia? |
| Sedes y contacto | Â¿DÃ³nde estÃ¡n las oficinas de atenciÃ³n?, Â¿CÃ³mo contactar soporte? |
| FacturaciÃ³n y pagos | Â¿DÃ³nde se pueden realizar pagos?, Â¿CÃ³mo consultar el estado de la factura? |
| Responsabilidad social | Â¿QuÃ© programas de sostenibilidad tiene Celsia? |
| Noticias e innovaciÃ³n | Â¿QuÃ© proyectos recientes ha desarrollado Celsia? |

---

## 2ï¸âƒ£ ConstrucciÃ³n de la Base de Conocimiento SemÃ¡ntico - Arquitectura Propuesta

![alt text](Diagrama.png)

La arquitectura propuesta se basa en un flujo secuencial de componentes que automatizan la adquisiciÃ³n, procesamiento, almacenamiento y consulta de informaciÃ³n. Cada mÃ³dulo cumple un rol especÃ­fico dentro del ecosistema del canal inteligente de Celsia:

### ğŸ”¹ Selenium â€“ ExtracciÃ³n AutomÃ¡tica de Datos  
El proceso se inicia con **Selenium**, una herramienta de automatizaciÃ³n web que permite realizar **web scraping controlado** sobre los portales oficiales de Celsia.  
Este componente navega por las pÃ¡ginas y extrae la informaciÃ³n relevante (por ejemplo, puntos de pago, noticias, procesos de facturaciÃ³n, tarifas, y servicios disponibles), garantizando que los datos estÃ©n siempre actualizados.

### ğŸ”¹ JSON â€“ EstructuraciÃ³n de la InformaciÃ³n  
Los datos obtenidos mediante Selenium se almacenan en **formato JSON**, lo que facilita su posterior manipulaciÃ³n, transporte e integraciÃ³n con otros mÃ³dulos del sistema.  
El formato JSON permite mantener una estructura clara y jerÃ¡rquica de la informaciÃ³n, representando eficientemente texto, categorÃ­as y metadatos.

### ğŸ”¹ Pandas â€“ Procesamiento y Limpieza de Datos  
Con el archivo JSON como entrada, **Pandas** se encarga de realizar la **limpieza, normalizaciÃ³n y estructuraciÃ³n tabular** de los datos.  
Este paso incluye la eliminaciÃ³n de duplicados, la estandarizaciÃ³n de campos y la organizaciÃ³n de los registros para su posterior indexaciÃ³n semÃ¡ntica.

### ğŸ”¹ ChromaDB â€“ Almacenamiento Vectorial SemÃ¡ntico  
Una vez procesada, la informaciÃ³n se transforma en **vectores embebidos** y se almacena en **ChromaDB**, una base de datos vectorial optimizada para bÃºsquedas semÃ¡nticas.  
Este componente permite que el asistente realice consultas basadas en el significado del texto (no solo coincidencias literales), mejorando la precisiÃ³n de las respuestas ante preguntas de los usuarios.

### ğŸ”¹ Ollama â€“ Modelo de Lenguaje Local  
**Ollama** actÃºa como el motor de inteligencia artificial local que ejecuta **modelos de lenguaje (LLMs)** preentrenados.  
Este modelo interpreta las consultas del usuario, accede a los vectores relevantes en ChromaDB y genera respuestas coherentes, naturales y ajustadas al contexto de la empresa.

### ğŸ”¹ LangChain â€“ OrquestaciÃ³n de Procesos y Consultas  
**LangChain** coordina la interacciÃ³n entre todos los mÃ³dulos del sistema.  
Define la lÃ³gica de flujo: recibe la consulta del usuario, consulta el vector store (ChromaDB), formatea la respuesta con el modelo Ollama y la devuelve al canal de comunicaciÃ³n (Streamlit).  
LangChain tambiÃ©n gestiona el *prompting* y los *retrieval chains*, garantizando consistencia y trazabilidad en las respuestas.

### ~~ğŸ”¹ Streamlit â€“ Interfaz de Usuario~~
~~Finalmente, **Streamlit** provee una interfaz web interactiva donde el usuario puede comunicarse con el asistente.~~  
~~El chat permite ingresar consultas en lenguaje natural y visualizar respuestas generadas dinÃ¡micamente por el modelo.~~  
~~AdemÃ¡s, Streamlit facilita el despliegue del sistema y su acceso desde diferentes dispositivos.~~


---

## âš¡ MigraciÃ³n y EvoluciÃ³n a FastAPI con Function Calling

Esta secciÃ³n detalla los cambios significativos realizados para evolucionar el proyecto, pasando de una interfaz local de Streamlit a una API REST robusta con FastAPI, potenciando la integraciÃ³n y la escalabilidad.

### ğŸ”¹ Contexto y JustificaciÃ³n de la MigraciÃ³n
La versiÃ³n inicial del agente operaba a travÃ©s de una interfaz Streamlit, adecuada para la demostraciÃ³n de la Capa de Conocimiento. Sin embargo, para su "productizaciÃ³n" y la integraciÃ³n con sistemas externos como n8n y plataformas de mensajerÃ­a (ej. WhatsApp), se requerÃ­a exponer el agente como un servicio web. Esto llevÃ³ a la migraciÃ³n a FastAPI y la adopciÃ³n de tÃ©cnicas avanzadas de orquestaciÃ³n.

### ğŸ”¹ Cambios Clave Implementados

1.  **De Streamlit a FastAPI (ProductizaciÃ³n):**
    *   El agente ahora reside en una **API REST construida con FastAPI**. Esto permite que servicios externos interactÃºen con el agente mediante solicitudes HTTP POST a un endpoint `/chat`.
    *   FastAPI gestiona la entrada de mensajes de usuario y un identificador de sesiÃ³n, devolviendo la respuesta del LLM en formato JSON.
    *   **Beneficio:** Facilita la integraciÃ³n con cualquier sistema externo (n8n, frontends personalizados, bots de mensajerÃ­a) y mejora la escalabilidad al desacoplar la interfaz del agente.

2.  **De Ollama Embeddings a GoogleGenerativeAIEmbeddings (Mejora de Embeddings):**
    *   El modelo de embeddings se ha cambiado de `nomic-embed-text` (vÃ­a Ollama) a `GoogleGenerativeAIEmbeddings` (`models/embedding-001`).
    *   **Impacto:** Para este cambio, fue crucial **regenerar completamente la base de datos vectorial ChromaDB**, ya que los diferentes modelos de embeddings generan espacios vectoriales incompatibles. El script `regenerate_chromadb.py` se encarga de este proceso.
    *   **Beneficio:** Acceso a modelos de embeddings de alta calidad de Google, potencialmente mejorando la relevancia y precisiÃ³n en la recuperaciÃ³n de documentos (RAG).

3.  **OrquestaciÃ³n con LangGraph y Function Calling (Fiabilidad y Flexibilidad):**
    *   La orquestaciÃ³n del agente ahora utiliza un `StateGraph` de LangGraph, permitiendo un control mÃ¡s granular sobre el flujo de ejecuciÃ³n y la gestiÃ³n del estado de la conversaciÃ³n.
    *   Se implementÃ³ **Function Calling (Structured Tool Calling)**. En lugar de depender de un `system_prompt` textual que guiaba al LLM en la selecciÃ³n de herramientas, ahora las herramientas se `bind_tools` directamente al LLM con esquemas Pydantic (`args_schema`). El LLM (Qwen3:4b) utiliza sus capacidades nativas de Function Calling para decidir cuÃ¡ndo y cÃ³mo usar cada herramienta, generando una salida JSON estructurada.
    *   **Beneficio:** Aumenta drÃ¡sticamente la fiabilidad en la invocaciÃ³n de herramientas, reduce alucinaciones relacionadas con la selecciÃ³n de herramientas y mejora la flexibilidad para aÃ±adir nuevas funcionalidades.

4.  **GestiÃ³n de Memoria y SesiÃ³n:**
    *   La API de FastAPI ahora maneja un `session_id` por cada conversaciÃ³n, utilizando un `InMemorySaver` de LangGraph. Esto permite mantener el contexto de la conversaciÃ³n para cada usuario, esencial para interacciones prolongadas.
    *   **Beneficio:** Soporte para mÃºltiples usuarios simultÃ¡neos, manteniendo el historial y la coherencia de la conversaciÃ³n.

5.  **Monitoreo y Observabilidad con LangSmith:**
    *   Se ha integrado **LangSmith** para proporcionar observabilidad completa del agente. Las trazas de cada interacciÃ³n (llamadas al LLM, uso de herramientas, pasos del grafo) se registran en LangSmith.
    *   **Beneficio:** Facilita la depuraciÃ³n, el anÃ¡lisis de rendimiento, la identificaciÃ³n de cuellos de botella y la mejora continua del comportamiento del agente.

---

## 3ï¸âƒ£ ConstrucciÃ³n del Aplicativo (VersiÃ³n FastAPI)

### ğŸ”¹ SelecciÃ³n del Modelo y Framework
Para la versiÃ³n actual del sistema Q&A se seleccionÃ³ la siguiente configuraciÃ³n:

| Componente | TecnologÃ­a Elegida | JustificaciÃ³n |
|-------------|--------------------|----------------|
| **Modelo LLM** | **QWEN 3 4B (Google, vÃ­a Ollama)** | Modelo open source liviano (4 billones de parÃ¡metros), optimizado para comprensiÃ³n y generaciÃ³n de texto en espaÃ±ol e inglÃ©s. Ofrece un excelente equilibrio entre **rendimiento y eficiencia computacional**, ideal para ejecuciÃ³n local o en entornos acadÃ©micos sin GPU de alto costo. Presenta baja tasa de alucinaciones y buen desempeÃ±o en tareas de **retrieval-based Q&A**. |
| **Framework API** | **FastAPI** | Framework moderno y de alto rendimiento para construir APIs REST, ideal para exponer el agente como un servicio web. |
| **OrquestaciÃ³n del Agente** | **LangGraph** | ExtensiÃ³n de LangChain que permite construir agentes robustos con manejo de estado y grafos de ejecuciÃ³n, facilitando la implementaciÃ³n de Function Calling. |
| **Embeddings** | `GoogleGenerativeAIEmbeddings` (`models/embedding-001`) | Embeddings de alta calidad de Google, utilizados para transformar el texto en vectores numÃ©ricos, mejorando la precisiÃ³n semÃ¡ntica. |
| **Base de datos vectorial** | **ChromaDB (open source)** | Ligera, eficiente y de integraciÃ³n directa con LangChain; ideal para almacenar y consultar embeddings de texto. |

---

### ğŸ”¹ Arquitectura General del Sistema RAG (FastAPI)
1. **Consulta del usuario** â†’ PeticiÃ³n POST a la API de FastAPI (`/chat`).  
2. **API FastAPI** â†’ Recibe la consulta y el `session_id`.  
3. **Agente LangGraph** â†’ Recibe la consulta, gestiona la memoria de la sesiÃ³n y decide si invocar una herramienta o el RAG.  
4. **BÃºsqueda semÃ¡ntica** (si se invoca el RAG) â†’ El texto de la pregunta se convierte en embedding (via Google Generative AI) y se compara con los embeddings de los chunks almacenados en ChromaDB.  
5. **RecuperaciÃ³n de contexto relevante** â†’ Se extraen los fragmentos mÃ¡s similares.  
6. **GeneraciÃ³n de respuesta** â†’ El modelo QWEN 3 4B utiliza el contexto recuperado (o el resultado de otra herramienta) para elaborar una respuesta precisa y contextualizada.  
7. **Respuesta API** â†’ La API de FastAPI devuelve la respuesta del LLM en formato JSON.

---

### ğŸ”¹ AplicaciÃ³n de Prompt Engineering
Se mantiene un prompt de sistema robusto para el componente RAG con las siguientes instrucciones (ahora definido en `src/agent/core.py`):

```
"""**[INSTRUCCIONES CLAVE ZERO-SHOT Y LIMITACIÃ“N DE FUENTE]**
Tu ÃšNICA tarea es responder a la **PREGUNTA** del usuario, utilizando EXCLUSIVAMENTE la informaciÃ³n que se encuentra en el **CONTEXTO** proporcionado a continuaciÃ³n.

**REGLAS ESTRICTAS para evitar alucinaciones:**
1.  **SI** la respuesta a la PREGUNTA se encuentra explÃ­cita o implÃ­citamente en el **CONTEXTO**, genera una respuesta completa y profesional.
2.  **SI** no puedes encontrar la respuesta en el **CONTEXTO**, o si la informaciÃ³n es insuficiente, debes responder **ÃšNICAMENTE** con la siguiente frase predefinida: "Lamento no poder ofrecer una respuesta precisa basada en la informaciÃ³n disponible. Por favor, consulta los canales oficiales de CELSIA o llama a la lÃ­nea de servicio al cliente."
3.  **NUNCA** utilices tu conocimiento general o informaciÃ³n que no estÃ© en el **CONTEXTO**. **NUNCA** inventes tarifas, fechas o procesos.
Coloca el cursor sobre un mensaje para fijarlo

Contexto:
{context}

Pregunta: {question}

Respuesta:"""
```

El contexto se completa dinÃ¡micamente con los fragmentos recuperados desde ChromaDB antes de cada consulta del usuario. La selecciÃ³n de herramientas para el agente principal se gestiona mediante **Function Calling** (ver secciÃ³n âš¡ MigraciÃ³n y EvoluciÃ³n a FastAPI).

---

## ğŸ“‚ Estructura del Proyecto (VersiÃ³n FastAPI)

La siguiente estructura de carpetas ha sido diseÃ±ada para una mayor modularidad, mantenibilidad y separaciÃ³n de preocupaciones.

```
/agent_celsia
â”œâ”€â”€ .env                  # Variables de entorno (API keys, URLs, configuraciones)
â”œâ”€â”€ .gitignore            # Archivos a ignorar por Git
â”œâ”€â”€ pyproject.toml        # GestiÃ³n de dependencias y metadatos del proyecto (Poetry/uv)
â”œâ”€â”€ uv.lock               # Bloqueo de dependencias de uv
â”œâ”€â”€ README.md             # DocumentaciÃ³n principal del proyecto
â”œâ”€â”€ main.py               # Punto de entrada para la aplicaciÃ³n FastAPI
â”œâ”€â”€ regenerate_chromadb.py # Script para (re)generar la base de datos ChromaDB
â”œâ”€â”€ test_api.py           # Pruebas para la API de FastAPI
â”œâ”€â”€ test_parametros_llm.py # Pruebas para parÃ¡metros del LLM
â”œâ”€â”€ scripts/              # Scripts auxiliares (e.g., para despliegue, mantenimiento)
â”‚   â”œâ”€â”€ run_ollama.sh     # Ejemplo: script para iniciar Ollama (si es relevante)
â”‚   â””â”€â”€ diagnostico_chromadb.py # Script de diagnÃ³stico para ChromaDB
â”œâ”€â”€ src/                  # CÃ³digo fuente de la aplicaciÃ³n
â”‚   â”œâ”€â”€ __init__.py       # Hace de 'src' un paquete Python
â”‚   â”œâ”€â”€ agent/            # LÃ³gica central del agente (LangGraph)
â”‚   â”‚   â”œâ”€â”€ __init__.py   # Inicializador de paquete
â”‚   â”‚   â”œâ”€â”€ core.py       # DefiniciÃ³n principal del agente, grafo, LLM, RAG
â”‚   â”‚   â””â”€â”€ state.py      # DefiniciÃ³n de AgentState (estado de la conversaciÃ³n)
â”‚   â”œâ”€â”€ data/             # MÃ³dulos para gestiÃ³n de datos y embeddings
â”‚   â”‚   â”œâ”€â”€ __init__.py   # Inicializador de paquete
â”‚   â”‚   â””â”€â”€ processing.py # LÃ³gica para cargar y preprocesar documentos (JSON -> Document)
â”‚   â”‚   â””â”€â”€ vectorstore.py# (Pendiente) Funciones para inicializar/manejar ChromaDB
â”‚   â”œâ”€â”€ models/           # Definiciones de modelos Pydantic para la API
â”‚   â”‚   â”œâ”€â”€ __init__.py   # Inicializador de paquete
â”‚   â”‚   â””â”€â”€ api_models.py # Modelos Pydantic para la API (ej. ChatRequest, ChatResponse)
â”‚   â”œâ”€â”€ tools/            # ImplementaciÃ³n de las herramientas que el agente puede usar
â”‚   â”‚   â”œâ”€â”€ __init__.py   # Inicializador de paquete
â”‚   â”‚   â””â”€â”€ celsia_tools.py # ImplementaciÃ³n de todas las funciones de las herramientas
â”‚   â””â”€â”€ utils/            # Utilidades varias (helpers, loggers, etc.)
â”‚       â”œâ”€â”€ __init__.py   # Inicializador de paquete
â”‚       â”œâ”€â”€ config.py     # Carga de configuraciÃ³n, variables de entorno
â”‚       â””â”€â”€ errors.py     # DefiniciÃ³n de excepciones personalizadas
â”œâ”€â”€ data/                 # Datos del proyecto (fuentes originales, ChromaDB)
â”‚   â”œâ”€â”€ source/           # Documentos fuente originales
â”‚   â”‚   â”œâ”€â”€ celsia_processed_20251015_223656_chunks.json
â”‚   â”‚   â””â”€â”€ post_celsia.json
â”‚   â”œâ”€â”€ chromadb_storage/ # Base de datos ChromaDB persistida
â”œâ”€â”€ docs/                 # DocumentaciÃ³n adicional (Arquitectura, n8n setup, etc.)
â”‚   â”œâ”€â”€ Arquitectura.drawio # Diagrama de arquitectura del sistema
â”‚   â”œâ”€â”€ Arquitectura.drawio.png # Imagen del diagrama
â”‚   â””â”€â”€ n8n_guide.md      # GuÃ­a detallada para la integraciÃ³n con n8n y WhatsApp
â”œâ”€â”€ notebooks/            # Jupyter Notebooks para experimentaciÃ³n, anÃ¡lisis de datos
â”‚   â”œâ”€â”€ rag_celsia.ipynb  # Notebook principal de RAG
â”‚   â”œâ”€â”€ transformation/   # Notebooks de transformaciÃ³n de datos
â”‚   â”‚   â”œâ”€â”€ celsia_chunks_linekedin.ipynb
â”‚   â”‚   â””â”€â”€ celsia_chunks_viewer.ipynb
â”‚   â””â”€â”€ web_scraping/     # Scripts/Notebooks de web scraping
â”‚       â”œâ”€â”€ celsia_linkedin_scraper.py
â”‚       â”œâ”€â”€ celsia_unified_scraper_dev.py
â”‚       â””â”€â”€ celsia_unified_scraper.py
â””â”€â”€ .venv/                # Entorno virtual de Python (gestionado por uv)
```

### ğŸ”¹ DescripciÃ³n de Componentes Clave:
*   **`main.py`**: Punto de entrada de la aplicaciÃ³n FastAPI. Inicializa la aplicaciÃ³n y carga el agente desde `src/agent/core.py`.
*   **`src/agent/core.py`**: Contiene la definiciÃ³n principal del agente, incluyendo la configuraciÃ³n del LLM, el grafo de LangGraph, las funciones de `call_model`, `call_tool`, y `should_continue`.
*   **`src/agent/state.py`**: Define el `AgentState` (TypedDict) que gestiona el estado de la conversaciÃ³n del agente.
*   **`src/tools/celsia_tools.py`**: Implementa todas las herramientas personalizadas que el agente puede invocar (ej. obtener telÃ©fono, reportar daÃ±o, buscar documentos).
*   **`src/data/processing.py`**: Contiene la lÃ³gica para cargar y preprocesar documentos JSON, transformÃ¡ndolos en objetos `Document` utilizables por LangChain.
*   **`regenerate_chromadb.py`**: Script independiente para (re)generar la base de datos vectorial ChromaDB, utilizando los documentos de `data/source` y los embeddings configurados.

---

## ğŸš€ GuÃ­a de Inicio RÃ¡pido

Sigue estos pasos para poner en marcha el Agente Celsia en tu entorno local.

1.  **Clonar el Repositorio:**
    ```bash
    git clone https://github.com/jonathandzrtg/agent_celsia
    cd tu_repositorio
    ```

2.  **Configurar Entorno Virtual e Instalar Dependencias:**
    Utilizaremos `uv` para la gestiÃ³n de dependencias, lo cual es mÃ¡s rÃ¡pido.
    ```bash
    uv venv # Crea el entorno virtual si no existe
    uv pip install -r requirements.txt
    ```

3.  **Configurar Variables de Entorno (`.env`):**
    Crea un archivo `.env` en la raÃ­z del proyecto y configura las siguientes variables. Sustituye los valores `YOUR_..._KEY` por tus credenciales reales.

    ```dotenv
    OLLAMA_LLM_MODEL=qwen3:4b
    OLLAMA_BASE_URL=http://localhost:11434
    
    GOOGLE_API_KEY=YOUR_GOOGLE_API_KEY
    
    # LangSmith Configuration (Optional but Recommended for Observability)
    LANGCHAIN_TRACING_V2=true
    LANGCHAIN_API_KEY=YOUR_LANGSMITH_API_KEY
    LANGCHAIN_PROJECT=Celsia Chatbot
    ```
    *AsegÃºrate de tener una `GOOGLE_API_KEY` vÃ¡lida con cuota suficiente para `GoogleGenerativeAIEmbeddings`.*

4.  **Iniciar Ollama y Descargar Modelos:**
    Si no tienes Ollama instalado, visita [https://ollama.com/download](https://ollama.com/download) para descargarlo.
    Abre una terminal y ejecuta:
    ```bash
    ollama serve
    ```
    En otra terminal, descarga los modelos necesarios:
    ```bash
    ollama pull qwen3:4b
    # (nomic-embed-text ya no es necesario para embeddings si usas Google)
    ```

5.  **Regenerar Base de Datos Vectorial (ChromaDB):**
    Este paso es CRÃTICO cada vez que cambias el modelo de embeddings o los documentos fuente. AsegÃºrate de que **no haya otro proceso (como el servidor FastAPI) accediendo a la carpeta `chromadb_storage`**.
    ```bash
    uv run regenerate_chromadb.py
    ```
    Esto crearÃ¡ o actualizarÃ¡ la base de datos vectorial con los embeddings de Google Generative AI.

6.  **Iniciar AplicaciÃ³n FastAPI:**
    Abre una terminal y ejecuta:
    ```bash
    uvicorn main:app --host 0.0.0.0 --port 8000
    ```
    Verifica que la consola muestre "âœ… Agent components loaded successfully."

7.  **Probar la API:**
    Abre tu navegador y ve a `http://localhost:8000/docs`. AquÃ­ encontrarÃ¡s la interfaz Swagger UI para probar los endpoints de tu API, incluyendo `/chat`.

---

## 4ï¸âƒ£ Pruebas (API) y DocumentaciÃ³n

### ğŸ”¹ Pruebas
Se implementaron pruebas de integraciÃ³n (`test_api.py`) para verificar la funcionalidad de la API de FastAPI. Aunque actualmente no pasan en el entorno de TestClient debido a la complejidad de la inicializaciÃ³n del LLM/Embeddings en un contexto de prueba aislado, la funcionalidad se ha validado manualmente.

### ğŸ”¹ AplicaciÃ³n de Prompt Engineering
Se diseÃ±Ã³ un prompt de sistema robusto para el componente RAG con las siguientes instrucciones (ahora definido en `src/agent/core.py`):

```
"""**[INSTRUCCIONES CLAVE ZERO-SHOT Y LIMITACIÃ“N DE FUENTE]**
Tu ÃšNICA tarea es responder a la **PREGUNTA** del usuario, utilizando EXCLUSIVAMENTE la informaciÃ³n que se encuentra en el **CONTEXTO** proporcionado a continuaciÃ³n.

**REGLAS ESTRICTAS para evitar alucinaciones:**
1.  **SI** la respuesta a la PREGUNTA se encuentra explÃ­cita o implÃ­citamente en el **CONTEXTO**, genera una respuesta completa y profesional.
2.  **SI** no puedes encontrar la respuesta en el **CONTEXTO**, o si la informaciÃ³n es insuficiente, debes responder **ÃšNICAMENTE** con la siguiente frase predefinida: "Lamento no poder ofrecer una respuesta precisa basada en la informaciÃ³n disponible. Por favor, consulta los canales oficiales de CELSIA o llama a la lÃ­nea de servicio al cliente."
3.  **NUNCA** utilices tu conocimiento general o informaciÃ³n que no estÃ© en el **CONTEXTO**. **NUNCA** inventes tarifas, fechas o procesos.
Coloca el cursor sobre un mensaje para fijarlo

Contexto:
{context}

Pregunta: {question}

Respuesta:"""
```

El contexto se completa dinÃ¡micamente con los fragmentos recuperados desde ChromaDB antes de cada consulta del usuario. La selecciÃ³n de herramientas para el agente principal se gestiona mediante **Function Calling** (ver secciÃ³n âš¡ MigraciÃ³n y EvoluciÃ³n a FastAPI).

### ğŸ”¹ DescripciÃ³n del Problema
Necesidad de un canal de comunicaciÃ³n automatizado y preciso para la empresa **CELSIA**, donde los usuarios puedan consultar informaciÃ³n de la empresa, puntos de pago, procesos de facturaciÃ³n y demÃ¡s servicios prestados por la entidad.

### ğŸ”¹ Planteamiento de la SoluciÃ³n
CreaciÃ³n de un **sistema Q&A basado en RAG** como nÃºcleo de un futuro chatbot, alimentado con informaciÃ³n extraÃ­da desde el sitio web y el perfil corporativo de LinkedIn de la empresa, ahora expuesto como una API REST.

### ğŸ”¹ PreparaciÃ³n de los Datos
Se realizÃ³ la extracciÃ³n de los datos utilizando **Selenium**, exportando los textos a **JSON**. Luego se ejecutÃ³ un proceso de limpieza (eliminaciÃ³n de HTML, sÃ­mbolos y espacios no relevantes) y segmentaciÃ³n en **chunks** almacenados en archivos JSON, conformando la base de conocimiento para la fase de modelado.

### ğŸ”¹ Modelado
El sistema combina tres componentes principales:
- **Modelo de Embeddings:** GoogleGenerativeAIEmbeddings (`models/embedding-001`).
- **LLM:** QWEN 3 4B (vÃ­a Ollama), modelo base para la generaciÃ³n de respuestas contextuales.
- **Base Vectorial:** ChromaDB, por su simplicidad, escalabilidad y compatibilidad con LangChain.

El diseÃ±o del prompt instruye al modelo a responder exclusivamente con base en la informaciÃ³n recuperada, manteniendo precisiÃ³n y coherencia.

### ğŸ”¹ Resultados
La funcionalidad del agente ha sido validada manualmente a travÃ©s de la interfaz Swagger UI de FastAPI. El sistema logra una precisiÃ³n satisfactoria en la recuperaciÃ³n de informaciÃ³n correcta y contextualizada, con respuestas coherentes y mÃ­nima tendencia a alucinaciones.

## ğŸ“„ Conclusiones

- Se logrÃ³ construir una **base de conocimiento estructurada y contextualizada** de Celsia a partir de fuentes oficiales.  
- La implementaciÃ³n del modelo **QWEN 3 4B** ofrece una soluciÃ³n **open source eficiente, reproducible y escalable**.  
- El enfoque **RAG con LangGraph y Function Calling** permite integrar bÃºsqueda semÃ¡ntica y generaciÃ³n natural de respuestas de manera robusta y fiable.  
- La migraciÃ³n a **FastAPI** productiza el agente, facilitando su integraciÃ³n con n8n y otros servicios, sentando las bases para futuras implementaciones como chatbots de WhatsApp.
