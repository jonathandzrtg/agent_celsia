"""
RAG Chat Interface con Streamlit
Interfaz sencilla para consultar la base de datos vectorial ChromaDB
"""

import streamlit as st
from langchain_chroma import Chroma
#from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain_ollama import OllamaLLM
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough

from langchain.agents import create_agent
from langgraph.checkpoint.memory import InMemorySaver 

# Tool
def get_tel_celsia():
    """Funcion para obtener el telefono de celsia"""
    return "3102226655"

#validar que ya existen hilos y no los borra
if "checkpointer" not in st.session_state:
    st.session_state.checkpointer = InMemorySaver()

if "thread_id" not in st.session_state:
    st.session_state.thread_id = "1"

# CONFIGURACI√ìN DE LA P√ÅGINA

st.set_page_config(
    page_title="Celsia Agent",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Titulo
st.markdown("<h1 style='text-align: center;'>ü§ñ Agente Celsia llama 3.2 3B</h1>", unsafe_allow_html=True)
st.write("En que puedo ayudarte hoy?")


# INICIALIZAR SESI√ìN


if "messages" not in st.session_state:
    st.session_state.messages = []

if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None

if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None


# CARGAR VECTORSTORE

@st.cache_resource
def cargar_rag():
    """Carga el vectorstore y la cadena RAG"""
    
    with st.spinner("‚è≥ Cargando base de datos vectorial..."):
        # Embeddings
        embeddings = OllamaEmbeddings(
            model="nomic-embed-text",
            base_url="http://localhost:11434"
        )
        
        # Cargar vectorstore desde ChromaDB
        vectorstore = Chroma(
            persist_directory="./chromadb_storage",
            embedding_function=embeddings,
            collection_name="rag_collection"
        )
        
        # LLM
        #llm = OllamaLLM(
        #    model="llama3.2:3b",
        #    base_url="http://localhost:11434",
        #    temperature=0.7
        #)

        llm = create_agent(
            model="ollama:llama3.2:3b",
            tools=[get_tel_celsia], #le pasamos al agente las tools que el puede usar, el las analiza segun el caso
            system_prompt="You are a helpful assistant",
            checkpointer=st.session_state.checkpointer,
        )
                
        # Retriever
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3}
        )
        
        # Prompt
        prompt_template = """**[INSTRUCCIONES CLAVE ZERO-SHOT Y LIMITACI√ìN DE FUENTE]**
Tu √öNICA tarea es responder a la **PREGUNTA** del usuario, utilizando EXCLUSIVAMENTE la informaci√≥n que se encuentra en el **CONTEXTO** proporcionado a continuaci√≥n.

**REGLAS ESTRICTAS para evitar alucinaciones:**
1.  **SI** la respuesta a la PREGUNTA se encuentra expl√≠cita o impl√≠citamente en el **CONTEXTO**, genera una respuesta completa y profesional.
2.  **SI** no puedes encontrar la respuesta en el **CONTEXTO**, o si la informaci√≥n es insuficiente, debes responder **√öNICAMENTE** con la siguiente frase predefinida: "Lamento no poder ofrecer una respuesta precisa basada en la informaci√≥n disponible. Por favor, consulta los canales oficiales de CELSIA o llama a la l√≠nea de servicio al cliente."
3.  **NUNCA** utilices tu conocimiento general o informaci√≥n que no est√© en el **CONTEXTO**. **NUNCA** inventes tarifas, fechas o procesos.
Coloca el cursor sobre un mensaje para fijarlo
        
Contexto:
{context}

Pregunta: {question}

Respuesta:"""
        
        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # Cadena RAG
        def formato_docs(docs):
            return "\n\n".join([doc.page_content for doc in docs])
        
        rag_chain = (
            {"context": retriever | formato_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
        )
        
        return vectorstore, rag_chain, retriever

# Cargar RAG
vectorstore, rag_chain, retriever = cargar_rag()

st.success("‚úÖ Base de datos cargada correctamente")

# SIDEBAR

with st.sidebar:
    st.header("‚öôÔ∏è Configuraci√≥n")
    
    # Info
    st.info(f"üìä Documentos en la BD: {vectorstore._collection.count()}")
    
    # Bot√≥n para limpiar historial
    if st.button("üóëÔ∏è Limpiar historial"):
        st.session_state.messages = []
        st.rerun()
    
    # Informaci√≥n
    st.divider()
    st.subheader("‚ÑπÔ∏è Informaci√≥n")
    st.write("""
    - **Modelo LLM**: Llama 3.2 3B
    - **Embeddings**: nomic-embed-text
    - **Base de datos**: ChromaDB
    - **Framework**: LangChain
    """)

# MOSTRAR HISTORIAL DE MENSAJES

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ENTRADA DEL USUARIO

pregunta = st.chat_input("Escribe tu pregunta...")

if pregunta:
    # Agregar pregunta al historial
    st.session_state.messages.append({"role": "user", "content": pregunta})
    
    with st.chat_message("user"):
        st.markdown(pregunta)
    
    # Generar respuesta
    with st.chat_message("assistant"):
        with st.spinner("‚è≥ Pensando..."):
            # Obtener respuesta
            respuesta = rag_chain.invoke(pregunta)
            
            # Mostrar respuesta
            st.markdown(respuesta)
            
            # Agregar al historial
            st.session_state.messages.append({
                "role": "assistant",
                "content": respuesta
            })
            
            # Mostrar documentos recuperados en expander
            with st.expander("üìö Documentos recuperados"):
                docs = retriever.invoke(pregunta)
                for i, doc in enumerate(docs, 1):
                    st.write(f"**[{i}] {doc.metadata['source']}**")
                    st.info(doc.page_content)